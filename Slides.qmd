---
title: "Exploration of Different Imputation Methods for Missing Data"
author: "Karthik Aerra, Liz Miller, Mohit Kumar Veeraboina, Robert Stairs"
date: '`r Sys.Date()`'
format:
  revealjs:
    theme: simple
    smaller: true
    scrollable: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Introduction

## Introduction

- Missing data in real-world datasets is a key challenge.
- Causes: Processing errors, measurement errors, survey non-responses, invalid calculations, participant dropout.
- Impact: Reduces usable observations, introduces bias, interferes with analysis tools.
- Importance: Accurate handling is critical for reliable data analysis.


## Identifying Missing Data

- **Representation:** Standard (NA, NaN, Null, None), strings (blanks, spaces, empty strings), numerical placeholders (extreme values like 999999).
- **Useful R Functions:** is.na() counts standard missing values, unique() identifies unexpected strings.
- **Visual Tools:** Histograms for numerical placeholders.
- **Example:** Survey dataset with missing age values represented as blanks.


## Missing Data Mechanisms

- **MCAR (Missing Completely at Random)**: Independent of observed and unobserved data. Example: Random network connectivity loss. Impact: Deletion reduces data size, no bias.
- **MAR (Missing at Random)**: Depends on observed data. Example: Age omission in surveys by specific demographics. Impact: Potential bias if not handled.
- **MNAR (Missing Not at Random)**: Related to unobserved data. Example: Health survey non-responses due to severity of illness. Impact: Complex analysis, prone to bias.


## Techniques for Handling Missing Data

**Deletion Methods:**

- Listwise Deletion: Removes rows with any missing values. Simple but reduces dataset size.
- Feature Selection: Removes columns with high proportions of missing data. Maintains observation count but reduces feature count.
- Example: Removing rows with missing income data in a financial dataset.

**Single Imputation:**

- Mean/Median Imputation: Replaces missing values with the mean/median of the column.
- Hot Deck Imputation: Uses values from similar records to fill missing data.
- Regression-based Imputation: Predicts missing values using regression models based on other variables.
- Example: Replacing missing temperature readings with the average temperature of the day.

**Multiple Imputation:**

- MICE (Multiple Imputation by Chained Equations): Iteratively imputes missing data by predicting and incorporating uncertainty.
- Process: Generates multiple datasets, imputes missing values, combines results for final analysis.
- Example: Using MICE to impute missing survey responses across several related questions.


## Theories for Missing Data

**Rubin’s Missing Data Theory:**

- Categories: MCAR, MAR, MNAR.
- Framework for understanding mechanisms behind missing data.
- Guides selection of imputation strategies based on these mechanisms.

**Statistical Theory of Imputation:**

- Uses statistical models to estimate missing values.
- Multiple imputation generates multiple plausible values for each missing data point.
- Example: Using multiple imputation to handle missing financial data in economic research.


**Machine Learning Theories:**

- Employ techniques like k-nearest neighbors (KNN) and neural networks.
- Predict missing values based on patterns in observed data.
- Example: Using KNN to fill in missing entries in a customer database.

## Research Methods for Missing Data

**Descriptive Studies:**

- Analyze patterns of missing data.
- Example: Jerez et al. analyzed missing data in medical records.

**Comparative Studies:**

- Evaluate the performance of different imputation methods.
- Example: Ibrahim and Zheng compared methods in clinical trial data.

**Simulation Studies:**

- Test imputation methods on controlled datasets.
- Allow systematic observation of the impact of different techniques.
- Example: Using simulated datasets to evaluate the accuracy of imputation methods.

**Application-Based Studies:**

- Real-world implementation examples.
- Example: Saqib Ejaz Awan demonstrated imputation methods in large-scale survey data.

# Overview of Methodology

![](Missing Data Workflow Graphic Updated.png)

## Classification of Data

![](missing_data.png){width="686"}

## Randomness Testing {.smaller}

::: columns
::: {.column width="50%"}
::: {style="font-size: 0.5em;"}
-   Little's MCAR Test:
-   Hawkin's Test
-   Non-Parametric Test
-   Data Pattern Visualizations
:::
:::

::: {.column width="100%"}
![](Screenshot 2024-07-28 135932.png){width="694"}
:::
:::

## Imputation Methods

::: columns
::: {.column width="50%"}
::: {style="font-size: 0.7em;"}
-   **Deletion**:
    -   List-wise
    -   Feature selection
:::
:::

::: {.column width="100%"}
![](40537_2020_313_Fig4_HTML.png){width="694"}
:::
:::

## Imputation Methods

::: columns
::: {.column width="50%"}
::: {style="font-size: 0.7em;"}
**Simple**:

-   Mean

-   Median

-   Mode
:::
:::

::: {.column width="50%"}
::: {style="font-size: 0.7em;"}
**Complex**:

-   MICE

-   missForest
:::
:::

::: {.column width="100%"}
![](10-1241508x11.png){width="620"}
:::
:::

## Model-Fitting: Decision Tree

![](Decision Tree Example.png){fig-align="center"}


## Model-Fitting - Random Forest

![](Random Forest Example.png){fig-align="center"}

------------------------------------------------------------------------

## Model-Fitting - KNN

![](KNN Example.png){fig-align="center"}

# Analysis and Results

## Introduction to the Dataset

- “Ozone: Los Angeles Ozone Pollution Data, 1976” from mlbench package in R ("Ozone")
- It contains observations related to pollution levels in the Los Angeles area during 1976.
- It contains a total of 13 variables, 366 observations (one for each day for one year)
- We chose this dataset due to the **high volume of already-missing data**
- Demonstration of a real-life scenario, where missing values are truly unknown and effectiveness of imputation methods cannot directly be measured.
- It is up to the investigator to choose a methodology for handling the missing data and appropriate metrics for evaluating effectiveness of missing data methods


## Variables in the Ozone Dataset

- V1: Month, 1-12, where 1 is January and 12 is December
- V2: Day of month
- V3: Day of week, 1-7, where 1 is Monday and 7 is Sunday
- V4:	Daily maximum one-hour-average ozone reading
- V5: 500 millibar pressure height (m) measured at Vandenberg AFB
- V6:	Wind speed (mph) at  Los Angeles International Airport (LAX)
- V7:	Humidity (%) at LAX
- V8:	Temperature (degrees F) measured at Sandburg, CA
- V9:	Temperature (degrees F) measured at El Monte, CA
- V10: Inversion base height (feet) at LAX
- V11: Pressure gradient (mm Hg) from LAX to Daggett, CA
- V12: Inversion base temperature (degrees F) at LAX
- V13: Visibility (miles) measured at LAX


```{r, warning=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(mlbench)
library(dplyr)
library(ggplot2)
library(vtable)
library(knitr)
library(kableExtra)
library(tidyverse)
library(tidyr)
library(naniar)
library(ggplot2)
library(UpSetR)
library(corrplot)
library(gridExtra)
library(reshape2)
library(ggpubr)
library(caTools)
library(party)
library(magrittr)
library(ggfittext)
```


```{r, warning=FALSE, echo=FALSE, include=FALSE}
# import data
data("Ozone", package = "mlbench")

# convert to data frame
ozone1 <- data.frame(Ozone)
# Reorder columns
ozone1 <- ozone1 %>%
  dplyr::select(V4,V1,V2,V3,V5,V6,V7,V8,V9,V10,V11,V12,V13)

# View summary of data
summary(ozone1)
```

## Data Pre-Processing Summary

- All columns converted to numeric data type
- Renamed columns for clearer interpretation during analysis and reporting
- Combined month, day of month and day of week into one date column

```{r, warning=FALSE, echo=FALSE, message=FALSE}
# Create function to check data types of data frame columns
check_data_types <- function(ozone1) {
  sapply(ozone1, class)
}

# Check data types of the columns
data_types <- check_data_types(ozone1)


# Function to convert specified columns from factor to numeric
convert_factors_to_numeric <- function(data, columns) {
  data[columns] <- lapply(data[columns], function(x) as.numeric(as.character(x)))
  return(data)
}

# Convert first 3 columns from factor to numeric
columns_to_convert <- c("V1", "V2", "V3")
ozone_date <- convert_factors_to_numeric(ozone1, columns_to_convert)
ozone2 <- convert_factors_to_numeric(ozone1, columns_to_convert)

# Check data types of the columns again
data_types <- sapply(ozone_date, class)

data_types2 <- sapply(ozone2, class)

```

```{r, warning=FALSE, echo=FALSE, message=FALSE}

# combine Day and Month to create a Date column
ozone_date <- ozone_date %>%
  mutate(Date = as.Date(paste(1976, V1, V2, sep = "-"), format = "%Y-%m-%d"))

```

```{r, warning=FALSE, echo=FALSE, message=FALSE}

# rename columns
ozone2 <- plyr::rename(ozone1, c('V4'="Ozone_reading",
                                 'V1'="Month", 
                                 'V2'="Day_of_month",
                                 'V3'="Day_of_week", 
                                 'V5'="Pressure_afb", 
                                 'V6'="Wind_speed_LAX", 
                                 'V7'="Humidity_LAX", 
                                 'V8'="Temp_sandburg", 
                                 'V9'="Temp_EM", 
                                 'V10'="IBH_LAX", 
                                 'V11'="Pressure_gradient", 
                                 'V12'="IBT_LAX", 
                                 'V13'="Visibility_LAX"))

ozone_date2 <- plyr::rename(ozone_date, c('V4'="Ozone_reading",
                                 'V1'="Month", 
                                 'V2'="Day_of_month",
                                 'V3'="Day_of_week", 
                                 'V5'="Pressure_afb", 
                                 'V6'="Wind_speed_LAX", 
                                 'V7'="Humidity_LAX", 
                                 'V8'="Temp_sandburg", 
                                 'V9'="Temp_EM", 
                                 'V10'="IBH_LAX", 
                                 'V11'="Pressure_gradient", 
                                 'V12'="IBT_LAX", 
                                 'V13'="Visibility_LAX"))
```


## Data Exploration Summary

- Summary statistics by day of week, month
- Histograms to visualize distributions of data 
- Correlation coefficients for all features with respect to Ozone levels (output)
- **Strong Positive Correlations:** Humidity_LAX, Pressure_afb, IBT_LAX, Temp_EM, and Temp_sandburg
- **Strong Negative Correlations:** IBH_LAX and Visibility_LAX


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}

# Summary statistics by day of the week 
ozone_summary_by_day <- ozone2 %>%
  group_by(Day_of_week) %>%
  summarize(
    mean_ozone = mean(Ozone_reading, na.rm = TRUE),
    median_ozone = median(Ozone_reading, na.rm = TRUE),
    max_ozone = max(Ozone_reading, na.rm = TRUE),
    min_ozone = min(Ozone_reading, na.rm = TRUE)
  )

```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}

ozone_summary_by_month <- ozone2 %>%
  group_by(Month) %>%
  summarize(
    mean_ozone = mean(Ozone_reading, na.rm = TRUE),
    median_ozone = median(Ozone_reading, na.rm = TRUE),
    max_ozone = max(Ozone_reading, na.rm = TRUE),
    min_ozone = min(Ozone_reading, na.rm = TRUE)
  )

```


```{r, warning=FALSE, echo=FALSE, message=FALSE}

# Make sure data is in numeric form
ozone2[] <- lapply(ozone2, as.numeric)

# Calculate correlations with Ozone
corr_coeffs <- cor(ozone2, use = "complete.obs")['Ozone_reading', ]
corr_coeffs <- corr_coeffs[!names(corr_coeffs) %in% 'Ozone_reading']

# Create a data frame for plotting
corr_df <- data.frame(Variable = names(corr_coeffs), Correlation = corr_coeffs)

# Create the bar graph
ggplot(corr_df, aes(x = reorder(Variable, Correlation), y = Correlation)) +
  geom_bar(stat = 'identity',fill = "blue") +
  xlab('Variable') +
  ylab('Correlation Coefficient') +
  ggtitle('Correlation Between Variables and Daily Average Ozone Reading') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}

## Humidity
# Create bins for humidity levels
ozone3 <- ozone2 %>%
  mutate(humidity_bin = cut(Humidity_LAX, breaks = seq(10, 100, by = 10), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data1 <- ozone3 %>%
  group_by(humidity_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data1, aes(x = humidity_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Humidity Levels",
       x = "Humidity Levels",
       y = "Percentage of Ozone Readings") +
  theme_minimal()


## Pressure
# Create bins for humidity levels
ozone3 <- ozone2 %>%
  mutate(pressure_bin = cut(Pressure_afb, breaks = seq(5300, 6000, by = 100), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data2 <- ozone3 %>%
  group_by(pressure_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data2, aes(x = pressure_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Pressure Levels",
       x = "Pressure Levels",
       y = "Percentage of Ozone Readings") +
  theme(axis.text.x = element_text(angle = 45, vjust=0.5, size=8))


## IBT - Inversion base temperature (degrees F) at LAX
# Create bins for Inversion base temp levels
ozone3 <- ozone2 %>%
  mutate(IBT_bin = cut(IBT_LAX, breaks = seq(20, 100, by = 10), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data3 <- ozone3 %>%
  group_by(IBT_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data3, aes(x = IBT_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Inversion base temp Levels",
       x = "Inversion base temp Levels",
       y = "Percentage of Ozone Readings") +
  theme_minimal()


## Temp_EM - Temperature (degrees F) measured at El Monte, CA
# Create bins for temp levels
ozone3 <- ozone2 %>%
  mutate(Temp_EM_bin = cut(Temp_EM, breaks = seq(20, 100, by = 10), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data4 <- ozone3 %>%
  group_by(Temp_EM_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data4, aes(x = Temp_EM_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Temp at El Monte Levels",
       x = "Temp Levels",
       y = "Percentage of Ozone Readings") +
  theme_minimal()


## Temp_sandburg
# Create bins for temp levels
ozone3 <- ozone2 %>%
  mutate(Temp_sd_bin = cut(Temp_sandburg, breaks = seq(20, 100, by = 10), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data5 <- ozone3 %>%
  group_by(Temp_sd_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data5, aes(x = Temp_sd_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Temp at Sandburg Levels",
       x = "Temp Levels",
       y = "Percentage of Ozone Readings") +
  theme_minimal()

```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}

## IBH_LAX - Inversion base height (feet) at LAX
# Create bins for IBH levels
ozone3 <- ozone2 %>%
  mutate(IBH_bin = cut(IBH_LAX, breaks = seq(100, 5500, by = 500), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data6 <- ozone3 %>%
  group_by(IBH_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data6, aes(x = IBH_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by IBH Levels",
       x = "IBH Levels",
       y = "Percentage of Ozone Readings") +
  theme(axis.text.x = element_text(angle = 45, vjust=0.5))


## Visibility
# Create bins for Visibility levels
ozone3 <- ozone2 %>%
  mutate(visibility_bin = cut(Visibility_LAX, breaks = seq(0, 500, by = 50), include.lowest = TRUE))

# Calculate percentage of ozone readings for each bin
percentage_data7 <- ozone3 %>%
  group_by(visibility_bin) %>%
  summarize(Ozone_reading = n()) %>%
  mutate(percentage = (Ozone_reading / sum(Ozone_reading)) * 100)

# Create the bar graph
ggplot(percentage_data7, aes(x = visibility_bin, y = percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Percentage of Ozone Readings by Visibility Levels",
       x = "Visibility Levels",
       y = "Percentage of Ozone Readings") +
  theme_minimal()

```



## Exploring Collinearity in the Dataset

- Correlation plot shows collinearity between several of the features in then dataset
- There are a number of predictors that show |r| > 0.5
- Temperature measurements and pressure notably have high collinearity


```{r, warning=FALSE, echo=FALSE, message=FALSE}
library("corrplot")

#Delete NA values to allow calculation of correlation coefficients
ozone_delete_NA <- na.omit(ozone2)


#Make the correlation matrix
ozone_cor = cor(ozone_delete_NA)


#Make the plot of the correlation matrix
corrplot(ozone_cor)

```


## Exploration of the Missing Data

- The total number of missing data points for each column is shown below as a count and as a percentage. 
- Most of the columns contain <5% missing values. Temp_EM contains 139 missing values, which is 38.0% of the observations. 
- The total number of missing values in the dataset is 203 out of 4,768 (13 columns times 366 observations). This represents 4.3% of the entire dataset. 
- There are a few instances where more than one column has missing data, but the majority of rows with missing values are only missing Temp_EM

```{r, warning=FALSE, echo=T, message=FALSE, include=FALSE}

# Function to summarize missing values in a data frame
summarize_missing_values <- function(data) {
  data %>%
    summarize_all(~ sum(is.na(.))) %>%
    gather(key = "column", value = "missing_values") %>%
    mutate(missing_percentage = (missing_values / nrow(data)) * 100)
}

missing_summary <- summarize_missing_values(ozone2)

# Print the summary of missing values
print(missing_summary)
print(sum(is.na(ozone2)))
```


```{r, warning=FALSE, echo=FALSE, message=FALSE}
# Plot missing data pattern
#Shows what percentage of the data are missing from each column
vis_miss(ozone2)
```

```{r, warning=FALSE, echo=FALSE, message=FALSE}
#Another way to visualize number of missing rows per column
gg_miss_var(ozone2) + labs(y = "Number of missing values") + ylim(0, 150)
```


```{r, warning=FALSE, echo=FALSE, message=FALSE}
#This plot gives a visual of what combinations of NAs are present and how many there are for each
#set nsets to 8 since we have 8 columns with missing data
gg_miss_upset(ozone2, nsets=8)
```


## Statistical Testing for MCAR (Missing Completely at Random)

- The MissMech package was used to test for MCAR
- *First, check for normality of dependent variable (ozone levels)*
- If normally distributed, use Hawkin's test
- If not normally distributed, use non-parametric test
- Shapiro-Wilk and Anderson Darling tests indicate non-normality (p < 0.05)
- Q-Q plot and histogram visibly show non-normal distribution
- **non-parametric test p-value: 0.27, fail to reject null hypothesis that data are MCAR**

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Perform Little's MCAR test
mcar_result <- mcar_test(ozone2)
print(mcar_result)
```

```{r, echo=FALSE, , include=FALSE}
# Extract the dependent variable
ozone_levels <- ozone2$Ozone_reading

# Remove NA values
ozone_levels <- na.omit(ozone_levels)

# Perform normality tests
library(nortest)
shapiro_test <- shapiro.test(ozone_levels)
ad_test <- ad.test(ozone_levels)

# Print the results
print(shapiro_test)
print(ad_test)
```
```{r, echo=FALSE}
# Histogram
ggplot(ozone2, aes(x = Ozone_reading)) + geom_histogram(binwidth = 5) + ggtitle("Histogram of Ozone")

# Q-Q plot
qqnorm(ozone2$Ozone_reading)
qqline(ozone2$Ozone_reading)
```

```{r, echo=FALSE, include=FALSE}
library(dplyr)
explanatory = c("Temp_EM","Month", "Day_of_month","Day_of_week", "Pressure_afb", "Wind_speed_LAX", "Humidity_LAX", "Temp_sandburg", "IBH_LAX", 
                "Pressure_gradient", "IBT_LAX", "Visibility_LAX")
dependent = "Ozone_reading"

# Run randomness test
ozone2 %>%
  select(explanatory)%>%
  MissMech::TestMCARNormality()

```

## Further Visualization of the Missingess Pattern

```{r, warning=FALSE, echo=FALSE, message=FALSE}

# Create gg_miss_fct plots with adjusted themes
p1 <- gg_miss_fct(ozone2, fct = Month) + 
  ggtitle("Missing Data by Month") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p2 <- gg_miss_fct(ozone2, fct = Day_of_month) + 
  ggtitle("Missing Data by Day of Month") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p3 <- gg_miss_fct(ozone2, fct = Day_of_week) + 
  ggtitle("Missing Data by Day of Week") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p4 <- gg_miss_fct(ozone2, fct = Ozone_reading) + 
  ggtitle("Missing Data by Ozone Reading") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p5 <- gg_miss_fct(ozone2, fct = Pressure_afb) + 
  ggtitle("Missing Data by Solar Radiation") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p6 <- gg_miss_fct(ozone2, fct = Wind_speed_LAX) + 
  ggtitle("Missing Data by Wind Speed") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p7 <- gg_miss_fct(ozone2, fct = Humidity_LAX) + 
  ggtitle("Missing Data by Humidity (LAX)") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p8 <- gg_miss_fct(ozone2, fct = Temp_sandburg) + 
  ggtitle("Missing Data by Temperature (Sandburg)") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(size=8))

p9 <- gg_miss_fct(ozone2, fct = Temp_EM) + 
  ggtitle("Missing Data by Temperature (EM)") +
  theme(plot.title = element_text(size=8))

p10 <- gg_miss_fct(ozone2, fct = IBH_LAX) + 
  ggtitle("Missing Data by IBH_LAX") +
  theme(plot.title = element_text(size=8))

p11 <- gg_miss_fct(ozone2, fct = Pressure_gradient) + 
  ggtitle("Missing Data by Pressure Gradient") +
  theme(plot.title = element_text(size=8))

p12 <- gg_miss_fct(ozone2, fct = IBT_LAX) + 
  ggtitle("Missing Data by IBT_LAX") +
  theme(plot.title = element_text(size=8))

p13 <- gg_miss_fct(ozone2, fct = Visibility_LAX) + 
  ggtitle("Missing Data by Visibility_LAX") +
  theme(plot.title = element_text(size=8))

# Arrange the plots into grids with proper spacing
grid1 <- grid.arrange(p1, p2, p3, p4, nrow = 2)
grid2 <- grid.arrange(p5, p6, p7, p8, nrow = 2)
grid3 <- grid.arrange(p9, p10, p11, nrow = 2)
grid4 <- grid.arrange(p12, p13, nrow = 1)

```

## Missing Data Deletion or Imputation

- Listwise deletion (delete rows with missing data)
- Feature selection (delete columns with >20% missing data) -> Temp_EM column
- Mean, median, or mode imputation
- MICE
- Random Forest
- A total of 7 datasets were created using various methods for handling missing data


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}

# Function to drop column if quantity of missing values is over the threshold
drop_na_columns <- function(data, threshold) {
  na_counts <- colSums(is.na(data))
  na_proportion <- na_counts / nrow(data)
  data <- data[, na_proportion <= threshold]
  return(data)
}
# Define threshold (e.g., 20% NA allowed)
threshold <- 0.20

# Drop columns based on the NA threshold
dropCol_data <- drop_na_columns(ozone2, threshold) # the column Temp_EM gets dropped
dropCol_data <- data.frame(dropCol_data)
dropCol_data <- na.omit(dropCol_data)

head(dropCol_data)
print(sum(is.na(dropCol_data)))
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Drop all missing values. Row deletion.
dropNA_data <- na.omit(ozone2)

head(dropNA_data)
print(sum(is.na(dropNA_data)))
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Function for mean imputation
mean_impute <- function(x) {
  x[is.na(x)] <- mean(x, na.rm = TRUE)
  return(x)
}

imputed_data_mean <- apply(ozone2, 2, mean_impute)
imputed_data_mean <- data.frame(imputed_data_mean)

head(imputed_data_mean)
print(sum(is.na(imputed_data_mean)))
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Function for median imputation
median_impute <- function(x) {
  x[is.na(x)] <- median(x, na.rm = TRUE)
  return(x)
}
imputed_data_median <- apply(ozone2, 2, median_impute)
imputed_data_median <- data.frame(imputed_data_median)

head(imputed_data_median)
print(sum(is.na(imputed_data_median)))
```


```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Function for mode imputation (using the most common value)
mode_impute <- function(x) {
  mode_val <- as.numeric(names(sort(table(x), decreasing = TRUE)[1]))
  x[is.na(x)] <- mode_val
  return(x)
}
imputed_data_mode <- apply(ozone2, 2, mode_impute)
imputed_data_mode <- data.frame(imputed_data_mode)

head(imputed_data_mode)
print(sum(is.na(imputed_data_mode)))
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, results=FALSE, include=FALSE}

library(missForest)

# verbose = If 'TRUE' the user is supplied with additional output between iterations
# xtrue = Complete data matrix
ozone2_mf <- missForest(ozone2, xtrue = ozone2, verbose = TRUE)
# convert back to data frame
ozone2_mf <- as.data.frame(ozone2_mf$ximp)
print(sum(is.na(ozone2_mf)))

## The final results can be accessed directly. The estimated error:
ozone2_mf$OOBerror

## The true imputation error (if available):
ozone2_mf$error
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, results=FALSE, include=FALSE}
library(mice)

# Impute missing values using MICE
# pmm = Predictive Mean Matching (suitable for continuous variables like temperature, wind, etc.)
# m = 5: number of imputed datasets to create.
# maxit = 50: max number of iterations
ozone2_mice <- mice(ozone2, method = "pmm", m = 5, maxit = 50)

# extracts the completed datasets from the mice object
ozone2_mice <- complete(ozone2_mice)

# Convert completed data to data frame
ozone2_mice <- as.data.frame(ozone2_mice)

print(sum(is.na(ozone2_mice)))
```

## Model-Fitting of Imputed Datasets

- Seven missing data methods -> Seven datasets
- Each dataset was split into train/test datasets using a split ratio of 0.75
- Models were fit for Ozone levels using random forest, decision tree, or KNN algorithms
- Resulting RMSE for test data was used as a metric for best missing data methodology

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
#Load additional libraries for model fitting
library(caret) # for fitting KNN models
library(e1071)
library(rsample) # for creating validation splits
library(recipes)    # for feature engineering
library(randomForest)
library(rpart)# decision tree
library(tidymodels) 
library(class) 
library(vip) 

# Split the data into training and testing sets
set.seed(123)
data_split_dropNA <- initial_split(dropNA_data, prop = 0.75)
train_dropNA <- training(data_split_dropNA)
test_dropNA <- testing(data_split_dropNA)

# Split data into predictors and target
X <- train_dropNA[, -1]  # Features
y <- train_dropNA$Ozone_reading  # Target

# Function to calculate RMSE
rmse <- function(pred, actual) {
  sqrt(mean((pred - actual)^2))
}
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Random forest model
rf_dropNA <- randomForest(Ozone_reading ~ ., data = train_dropNA)
# make predictions
rf_dropNA_pred <- predict(rf_dropNA, newdata = test_dropNA)
# Plot variable importance
varImpPlot(rf_dropNA, main = "Variable Importance Plot for Random Forest Model")

rf_dropNA_rmse <- rmse(rf_dropNA_pred, test_dropNA$Ozone_reading)
cat("Random Forest RMSE:", rf_dropNA_rmse, "\n")
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# KNN model
knn_dropNA <- train(Ozone_reading ~ ., data = train_dropNA, method = "knn")
# make predictions
knn_dropNA_pred <- predict(knn_dropNA, newdata = test_dropNA)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_dropNA_rmse <- rmse(knn_dropNA_pred, test_dropNA$Ozone_reading)
cat("KNN RMSE:", knn_dropNA_rmse, "\n")
```

```{r, warning=FALSE, echo=FALSE, message=FALSE, include=FALSE}
# Decision tree model
tree_dropNA <- rpart(Ozone_reading ~ ., data = train_dropNA)
# make predictions
tree_dropNA_pred <- predict(tree_dropNA, newdata = test_dropNA)
# Plot variable importance
var_importance <- varImp(tree_dropNA)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_dropNA_rmse <- rmse(tree_dropNA_pred, test_dropNA$Ozone_reading)
cat("Decision Tree RMSE:", tree_dropNA_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_dropCol <- initial_split(dropCol_data, prop = 0.75)
train_dropCol <- training(data_split_dropCol)
test_dropCol <- testing(data_split_dropCol)

# Split data into predictors and target
X <- train_dropCol[, -1]  # Features
y <- train_dropCol$Ozone_reading  # Target
```


```{r, include=FALSE, echo=FALSE}
# Random forest model
rf_dropCol <- randomForest(Ozone_reading ~ ., data = train_dropCol)
# make predictions
rf_dropCol_pred <- predict(rf_dropCol, newdata = test_dropCol)
# Plot variable importance
varImpPlot(rf_dropCol, main = "Variable Importance Plot for Random Forest Model")

rf_dropCol_rmse <- rmse(rf_dropCol_pred, test_dropCol$Ozone_reading)
cat("Random Forest RMSE:", rf_dropCol_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_dropCol <- train(Ozone_reading ~ ., data = train_dropCol, method = "knn")
# make predictions
knn_dropCol_pred <- predict(knn_dropCol, newdata = test_dropCol)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_dropCol_rmse <- rmse(knn_dropCol_pred, test_dropCol$Ozone_reading)
cat("KNN RMSE:", knn_dropCol_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_dropCol <- rpart(Ozone_reading ~ ., data = train_dropCol)
# make predictions
tree_dropCol_pred <- predict(tree_dropCol, newdata = test_dropCol)
# Plot variable importance
var_importance <- varImp(tree_dropCol)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_dropCol_rmse <- rmse(tree_dropCol_pred, test_dropCol$Ozone_reading)
cat("Decision Tree RMSE:", tree_dropCol_rmse, "\n")
```


```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_mean <- initial_split(imputed_data_mean, prop = 0.75)
train_mean <- training(data_split_mean)
test_mean <- testing(data_split_mean)

# Split data into predictors and target
X <- train_mean[, -1]  # Features
y <- train_mean$Ozone_reading  # Target

# Random forest model
rf_mean <- randomForest(Ozone_reading ~ ., data = train_mean)
# make predictions
rf_mean_pred <- predict(rf_mean, newdata = test_mean)
# Plot variable importance
varImpPlot(rf_mean, main = "Variable Importance Plot for Random Forest Model")

rf_mean_rmse <- rmse(rf_mean_pred, test_mean$Ozone_reading)
cat("Random Forest RMSE:", rf_mean_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_mean <- train(Ozone_reading ~ ., data = train_mean, method = "knn")
# make predictions
knn_mean_pred <- predict(knn_mean, newdata = test_mean)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_mean_rmse <- rmse(knn_mean_pred, test_mean$Ozone_reading)
cat("KNN RMSE:", knn_mean_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_mean <- rpart(Ozone_reading ~ ., data = train_mean)
# make predictions
tree_mean_pred <- predict(tree_mean, newdata = test_mean)
# Plot variable importance
var_importance <- varImp(tree_mean)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_mean_rmse <- rmse(tree_mean_pred, test_mean$Ozone_reading)
cat("Decision Tree RMSE:", tree_mean_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_med <- initial_split(imputed_data_median, prop = 0.75)
train_med <- training(data_split_med)
test_med <- testing(data_split_med)
```

```{r, include=FALSE, echo=FALSE}
# Random forest model
rf_med <- randomForest(Ozone_reading ~ ., data = train_med)
rf_med_pred <- predict(rf_med, newdata = test_med)
# Plot variable importance
varImpPlot(rf_med, main = "Variable Importance Plot for Random Forest Model")

rf_med_rmse <- rmse(rf_med_pred, test_med$Ozone_reading)
cat("Random Forest RMSE:", rf_med_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_med <- train(Ozone_reading ~ ., data = train_med, method = "knn")
knn_med_pred <- predict(knn_med, newdata = test_med)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_med_rmse <- rmse(knn_med_pred, test_med$Ozone_reading)
cat("KNN RMSE:", knn_med_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_med <- rpart(Ozone_reading ~ ., data = train_med)
tree_med_pred <- predict(tree_med, newdata = test_med)
# Plot variable importance
var_importance <- varImp(tree_med)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_med_rmse <- rmse(tree_med_pred, test_med$Ozone_reading)
cat("Decision Tree RMSE:", tree_med_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_mode <- initial_split(imputed_data_mode, prop = 0.75)
train_mode <- training(data_split_mode)
test_mode <- testing(data_split_mode)
```

```{r, include=FALSE, echo=FALSE}
# Random forest model
rf_mode <- randomForest(Ozone_reading ~ ., data = train_mode)
rf_mode_pred <- predict(rf_mode, newdata = test_mode)
# Plot variable importance
varImpPlot(rf_mode, main = "Variable Importance Plot for Random Forest Model")

rf_mode_rmse <- rmse(rf_mode_pred, test_mode$Ozone_reading)
cat("Random Forest RMSE:", rf_mode_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_mode <- train(Ozone_reading ~ ., data = train_mode, method = "knn")
knn_mode_pred <- predict(knn_mode, newdata = test_mode)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_mode_rmse <- rmse(knn_mode_pred, test_mode$Ozone_reading)
cat("KNN RMSE:", knn_mode_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_mode <- rpart(Ozone_reading ~ ., data = train_mode)
tree_mode_pred <- predict(tree_mode, newdata = test_mode)
# Plot variable importance
var_importance <- varImp(tree_mode)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_mode_rmse <- rmse(tree_mode_pred, test_mode$Ozone_reading)
cat("Decision Tree RMSE:", tree_mode_rmse, "\n")
```


```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_miss <- initial_split(ozone2_mf, prop = 0.75)
train_miss <- training(data_split_miss)
test_miss <- testing(data_split_miss)
```

```{r, include=FALSE, echo=FALSE}
# Random forest model
rf_miss <- randomForest(Ozone_reading ~ ., data = train_miss)
rf_miss_pred <- predict(rf_miss, newdata = test_miss)
# Plot variable importance
varImpPlot(rf_miss, main = "Variable Importance Plot for Random Forest Model")

rf_miss_rmse <- rmse(rf_miss_pred, test_miss$Ozone_reading)
cat("Random Forest RMSE:", rf_miss_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_miss <- train(Ozone_reading ~ ., data = train_miss, method = "knn")
knn_miss_pred <- predict(knn_miss, newdata = test_miss)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_miss_rmse <- rmse(knn_miss_pred, test_miss$Ozone_reading)
cat("KNN RMSE:", knn_miss_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_miss <- rpart(Ozone_reading ~ ., data = train_miss)
tree_miss_pred <- predict(tree_miss, newdata = test_miss)
# Plot variable importance
var_importance <- varImp(tree_miss)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_miss_rmse <- rmse(tree_miss_pred, test_miss$Ozone_reading)
cat("Decision Tree RMSE:", tree_miss_rmse, "\n")
```


```{r, include=FALSE, echo=FALSE}
# Split the data into training and testing sets
set.seed(123)
data_split_mice <- initial_split(ozone2_mice, prop = 0.75)
train_mice <- training(data_split_mice)
test_mice <- testing(data_split_mice)
```

```{r, include=FALSE, echo=FALSE}
# Random forest model
rf_mice <- randomForest(Ozone_reading ~ ., data = train_mice)
rf_mice_pred <- predict(rf_mice, newdata = test_mice)
# Plot variable importance
varImpPlot(rf_mice, main = "Variable Importance Plot for Random Forest Model")

rf_mice_rmse <- rmse(rf_mice_pred, test_mice$Ozone_reading)
cat("Random Forest RMSE:", rf_mice_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# KNN model
knn_mice <- train(Ozone_reading ~ ., data = train_mice, method = "knn")
knn_mice_pred <- predict(knn_mice, newdata = test_mice)
# Plot variable importance
feature_importance <- table(y) / length(y)
barplot(feature_importance, main = "Feature Importance for KNN", xlab = "Feature", ylab = "Importance")

knn_mice_rmse <- rmse(knn_mice_pred, test_mice$Ozone_reading)
cat("KNN RMSE:", knn_mice_rmse, "\n")
```

```{r, include=FALSE, echo=FALSE}
# Decision tree model
tree_mice <- rpart(Ozone_reading ~ ., data = train_mice)
tree_mice_pred <- predict(tree_mice, newdata = test_mice)
# Plot variable importance
var_importance <- varImp(tree_mice)
barplot(var_importance$Overall, main = "Variable Importance for Decision Tree", xlab = "Variable", ylab = "Importance")

tree_mice_rmse <- rmse(tree_mice_pred, test_mice$Ozone_reading)
cat("Decision Tree RMSE:", tree_mice_rmse, "\n")
```

## Model Performance for Each Missing Data Methodology

- The RMSE for each model and imputation method combination was summarized into a data frame
- The best model fit (lowest RMSE) was obtained when using **feature selection (column deletion)** for missing data and the **random forest** algorithm for model training
- Simple methods likely outperformed more advanced methods in this case since statistical testing supports that data are MCAR
- *There is no single best method for handling missing data, it will always depend on the context*
- Testing of multiple methods is the typical approach

```{r, warning=FALSE, echo=FALSE, message=FALSE}
models <- c('RandomForest', 'KNN', 'DecisionTree')
scores <- c(rf_dropCol_rmse, knn_dropCol_rmse, tree_dropCol_rmse,
            rf_dropNA_rmse, knn_dropNA_rmse, tree_dropNA_rmse,
            rf_mean_rmse, knn_mean_rmse, tree_mean_rmse,
            rf_med_rmse, knn_med_rmse, tree_med_rmse,
            rf_mode_rmse, knn_mode_rmse, tree_mode_rmse,
            rf_miss_rmse, knn_miss_rmse, tree_miss_rmse,
            rf_mice_rmse, knn_mice_rmse, tree_mice_rmse
)
ImpMethod <- c('DropCol','DropNA','Mean', 'Median', 'Mode','missForest','MICE')

# Create dataframe
rmse_df <- data.frame(Model = models, ImpMethod=ImpMethod,RMSE = scores)
print(rmse_df[order(rmse_df$RMSE), ])

```


## Model Performance for Each Missing Data Methodology

```{r, warning=FALSE, echo=FALSE, message=FALSE}
#Add a column that combines the imputation method and model fitting methods into one string

rmse_df$Combined_Methods <- paste(rmse_df$ImpMethod, rmse_df$Model)

#Create a bar chart
library(forcats)
library(ggplot2)
library(dplyr)

#Make a bar chart
RMSE_summary <- rmse_df %>%
  mutate(Combined_Methods = fct_reorder(Combined_Methods, desc(RMSE))) %>%
  ggplot (aes(x=Combined_Methods, y=RMSE)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  xlab("") + 
  theme_bw()

RMSE_summary

```


# Conclusions

## Conclusions

- In analytical processes, managing missing data is essential to preserving data integrity.
- We investigated a number of approaches to deal with missing data, such as imputation and deletion algorithms.
- Imputation techniques included mean, median, mode, random forest, MICE 
- Deletion methods included listwise (row deletion) and feature selection (column deletion)
- It is necessary to comprehend the missing data mechanism (MCAR, MAR, MNAR) in order to choose the best imputation techniques.

## Conclusions Continued 

- The efficacy of every imputation technique was evaluated using evaluation measures such as RMSE and R-Squared on the Ozone dataset.
- The features of the dataset and the goals of the analysis should guide the choice of imputation technique.
- When deletion procedures are used and the missing data are not entirely random, skewed conclusions may arise.
- While single imputation techniques are straightforward, they may understate data variability.
- Subsequent investigations may delve deeper into sophisticated machine learning methodologies to enhance the precision of missing data imputation.
