---
title: "STA6257 Capstone Project - Missing Data and Imputation "
author: "Karthik Aerra, Liz Miller, Mohit Kumar Veeraboina, Robert Stairs"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Missing data is a key challenge that data professionals encounter in their daily work. In the real world, data is often messy, incomplete, and requires a level of pre-processing before data analysis can occur. Part of data pre-processing is dealing with missing data. Missing data can occur for several reasons including but not limited to processing error, machine or measurement errors, non-responses in surveys, invalid calculations (e.g. division by zero), and study participant dropout [@Emmanuel2021surveyml]. Missing data can be problematic for analysis because it can reduce the number of usable observations, introduce bias, and interfere with different analysis tools such as machine learning algorithms, which cannot automatically handle missing data.

One of the first steps in handling missing data is to determine how much of the data is missing and from which columns. Missing data in a dataset can be represented in several ways including NA, NaN, Null, None, blanks, spaces, empty strings, or placeholder values such as 999999 (or equivalent obvious numerical outlier). For standard missing values such as NAs, functions such as is.na() in R can be used to count the number of missing observations. For string-related missing values such as space (“ ” or empty strings (“”), functions such as unique() in R can be used to identify unexpected strings. For numerical placeholders such as 0, 99999 (or equivalent), or negative values, histograms of the data can be useful to identify missing values.

If a dataset contains missing data, there are two basic approaches: deletion or imputation [@schefer2002dealingwithmissingdata]. Deletion of data can be problematic for analysis, especially when the proportion of missing data is high or the overall number of observations is low. Deletion of data can also lead to bias or understimation of variance if the data are not missing completely at random. It is crucial when choosing an approach to missing data that the underlying mechanism(s) for the missing data are understood. 

The mechanisms for missing data can generally be broken up into three categories [@mack2018managing], [@may2009modelling], [@bennett2001can], [@little2014joys], [@du2020missing]:

**Missing completely at random (MCAR)**
When data are MCAR, the probability of a record missing is independent from observed and unobserved data. For example, if performing a survey with age as a variable, missing values for age are random with respect to age and do not depend on other variables such as weight or gender. In the case of MCAR data, the deletion of data reduces the number of observations in the dataset but does not introduce bias. MCAR is the most ideal, but least realistic scenario.

**Missing at Random (MAR)**
When data are MAR, the probability of a record missing depends on the observed data, but not the unobserved data. Using the same example of a survey with age as a variable, people who are overweight may be more likely to omit their age in the survey. However, the probability of a missing data point can be attributed to another observed variable. In the case of MAR, deletion of entire observations with missing data may or may not introduce bias.

**Not missing at random (NMAR)**
When data are NMAR, the probability of a record missing is dependent on the unobserved data. In the same example of a survey including age as a variable, elderly people may be less likely to include their age in the survey. The missing value for age cannot be predicted using other variables included in the dataset such as weight or gender. There may be a factor that can predict the probability of a missing value, but it is unknown. NMAR data are the most complicated to analyze. Similarly to MAR, deletion of observations with missing data may or may not introduce bias to the analysis.

![MCAR, MAR, and NMAR Example, [@du2020missing]](Missing Data Mechanisms Figure.png)

Understanding the mechanism behind missing data (MCAR, MAR, or NMAR) is important because many imputation methods require the assumption of MCAR or MAR to be held true. There is not a statistical test or analysis to classify data as MCAR, MAR, or NMAR. Instead, visualization of the missing data is required to determine the relationships (or lack thereof) within the missing data. For example, correlation matrix plots, scatterplots, and other custom visualizations can be useful tools in identifying patterns in the missing data [@ghazali2020missing]. 

Imputation methods play a crucial role in addressing missing data, ranging from simple techniques like mean, median, and mode imputation to more sophisticated methods such as multiple imputation and machine learning approaches. Advanced methods generally offer improved accuracy but require careful consideration of underlying assumptions and computational resources.

Mean imputation involves replacing missing values with the average of observed values, assuming missingness is random. It's simple to implement but may underestimate variability. Median imputation, substituting missing values with the median, is robust to outliers and appropriate for skewed distributions. Mode imputation replaces missing values with the most frequently observed value, suitable for categorical data but ignoring variability. These methods are straightforward yet assume missing data are completely random, potentially biasing results. Advanced techniques like multiple imputation and machine learning offer more nuanced approaches, enhancing accuracy by considering complex relationships within datasets.

**Theories**

**Rubin's Missing Data Theory:** 
This foundational theory categorizes missing data into MCAR, MAR, and NMAR. It provides a framework for understanding the mechanisms behind missing data and guides the selection of appropriate imputation strategies based on these mechanisms. 

**Statistical Theory of Imputation:**
This theory involves using statistical models to estimate missing values, leveraging the relationship between observed and missing data points. Multiple imputation, a prominent technique within this theory, generates multiple plausible values for each missing data point to account for uncertainty. 

**Machine Learning Theories:**
Machine learning techniques are increasingly employed for imputing missing data due to their ability to learn patterns from data and make predictions. Methods such as k-nearest neighbors (KNN) and neural networks are applied to predict missing values based on observed data patterns.

These theories provide a comprehensive framework for understanding and implementing imputation methods across various datasets and analytical contexts.

**Research Methods:**

**Descriptive Studies:** These studies analyze the patterns of missing data and evaluate the effectiveness of various imputation methods. Examples include the work by Jerez et al. and Güzel [@jerez2010statisticalandmlmethods], [@guzel2013knn].

**Comparative Studies:** These studies compare the performance of different imputation methods. Joseph G. Ibrahim and Siming Zheng have conducted such comparative analyses, highlighting the strengths and weaknesses of various approaches [@ibrahim2011glm], [@zhang2024matrixcompletionmethod], [@zheng2023likelihood].

**Simulation Studies:** These studies test imputation methods on controlled datasets, allowing researchers to systematically observe the impact of different techniques. This helps in understanding the conditions under which each method performs best.

**Application-Based Studies:** These studies apply imputation methods to real-world data, demonstrating their practical use and the challenges involved. Examples include the works by Saqib Ejaz Awan and Emmanuel, which show how imputation methods are implemented in practical scenarios [@awan2022reinforcementlearning], [@Emmanuel2021surveyml].

**Theoretical Analysis:** This involves discussing the conditions, limitations, and best practices of imputation methods. Rachael A Hughes' work is an example, providing insights into when multiple imputation is not suitable and suggesting alternative approaches.


**Practical Implications of Advanced Imputation Methods for Missing Data**

1.   Improved Decision-Making in Data Analysis: 

Researchers and analysts may select the best imputation techniques to ensure the correctness of their studies by having a thorough understanding of the effects of various missing data types (MCAR, MAR, and NMAR) on datasets.
 
 
2.   Choosing the Best Imputation Techniques:

More sophisticated imputation techniques, such as matrix completion, K-Nearest Neighbors (KNN), Expectation-Maximization (EM), Multiple Imputation (MI) utilizing MCMC, and missForest, perform better at preserving dataset integrity. These approaches should be chosen by practitioners over easier ones, especially when handling MAR and NMAR data.

 
3.   Use in Medical and Clinical Settings:

Machine learning-based imputation techniques like Multi-Layer Perceptron (MLP) and KNN increase the accuracy of prediction models for medical practitioners dealing with datasets such as breast cancer prognosis. Improved patient outcomes and more informed healthcare decisions result from this.
 

4.   Improvement of Diagnostic Tools: 
In the medical domain in particular, combining imputation techniques (e.g., KNN) with classification algorithms (e.g., Naïve Bayes) improves diagnostic accuracy and lowers costs by eliminating pointless operations.

 
5.   Integration with Statistical and Analytical Software: 

These studies' insights direct the choice of suitable statistical software modules and functions (e.g., SPSS, SOLAS), maximizing data preprocessing and analysis.

 
6.   Robustness and Versatility Across Domains:

Techniques like missForest are robust and relevant in a variety of domains, including bioinformatics, industrial processes, and the social sciences. They handle mixed-type data without making significant distributional assumptions.

 
7.   Scalability and Practicality: 

Although techniques like matrix completion and imputation based on reinforcement learning provide great accuracy, they demand a large amount of computer power. Companies must weigh the advantages of these techniques against their computational expenses.
 

8.   Training and Expertise Requirements: 

Significant training and expertise in machine learning and statistics are required for the implementation and optimization of advanced imputation techniques. To fully utilize these strategies, organizations need spend money on professional development programs for staff members or on employing qualified experts.

 
9.   Context-Specific Method Selection: 

There isn't a single best imputation technique. Educators should try out several approaches to find the best match for their unique datasets and patterns of missing data.

 
10.Enhancement of Data Quality and Analytical Results: 
Higher-quality imputed datasets are produced by using sophisticated imputation techniques, which preserve the integrity of original data correlations and patterns. This improves the validity and dependability of later analysis and prediction models.
 
Practitioners in a variety of fields can greatly increase the accuracy and robustness of their analysis by implementing these sophisticated imputation approaches, which will improve decision-making and results.


### Related work



## Methods

**Article Selection Criteria and Search Strategy**

The approach used to select and analyze the literature included in this review involved identifying articles relevant to the topic of missing data and the imputation thereof.  Articles were selected based on their topic relevance as it related to statistical and machine learning concepts.  Preference was given to articles that were published recently to ensure the inclusion of the latest methods and research in the field.  In addition, articles were prioritized from peer-reviewed and reputable conference papers to ensure quality and reliability of the research.  A comprehensive search using Google Scholar and the available databases accessed through the University of West Florida was implemented.  Key search terms included: ‘missing data’, ‘missing data imputation’.

**Selection Criteria**

Articles were selected based on their relevance to the subject, in addition to including papers with a variety of methods implemented. Research addressing both statistical and machine learning approaches were focused on.  Articles not available in full text, not directly related to missing data imputation methods, or where the primary focus was theoretical as opposed to practical were excluded.


**Data and Analysis**

Once an article was selected, it was reviewed for relevant data pertaining to missing data imputation methods. The context of the study, the datasets involved, and the results provided were also reviewed. The article analysis focused on:


**Goal** – understanding the specific goal of the study and how it relates to missing data imputation.


**Methods/Techniques** – Identifying the specific methods used, whether statistical or machine learning, or a combination of both.


**Results/Applications** – evaluating the effectiveness/efficiency of the methods employed. Showing how these methods were/can be applied in a real-world scenario.


**Limitations/Challenges** – noting the limitations or challenges involved in each method used.


**Data** – identifying the types of data used for testing (e.g. simulated, real-world) and the type of missing data (e.g. missing at random, missing completely at random, not missing at random) 


## Analysis and Results

### Data and Visualization

**Description of the Dataset**

The “Ozone: Los Angeles Ozone Pollution Data, 1976” dataset has been chosen for this project. It contains observations related to pollution levels in the Los Angeles area during 1976. The variables contained in the dataset are thought to influence ozone concentration. These include daily ozone measurements consisting of: 
Ozone: response variable, daily max one-hour-average ozone concentration in parts per billion
Solar.R: solar radiation in Langley units
Wind: Average wind speed in mph
Temp: Max daily temp in degrees Fahrenheit
Month: month of observation
Day: day of the month of the observation

The dataset allows for analysis of how different meteorological conditions/factors can affect ozone levels. It contains a total of 13 variables, 366 observations (one for each day for one year), and 139 missing or NA values, which we will attempt to impute with the method chosen.



```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 

```

```{r, warning=FALSE, echo=TRUE}
# Load Data

```

### Statistical Modeling

```{r, warning=FALSE, echo=T, message=FALSE}

```

### Conclusion

## References
